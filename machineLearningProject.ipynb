{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c80240",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1b5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.exceptions import ConvergenceWarning # Ignoring convergence warnings (not necessarily required, but makes terminal more readable)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # As to not clutter the terminal with warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df062c",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b0d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and hyperparameter spaces in one dictionary. Easier to access and makes code more readable\n",
    "models = {\n",
    "    # Just using the suggested values here from the rubric\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=2000), {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }),\n",
    "    \"Random Forest\": (RandomForestClassifier(), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }),\n",
    "    \"Gaussian NB\": (GaussianNB(), {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    }),\n",
    "    \"SVM\": (SVC(probability=True), {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto', 0.1]\n",
    "    }),\n",
    "    \"KNN\": (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    }),\n",
    "    \"AdaBoost\": (AdaBoostClassifier(), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1]\n",
    "    }),\n",
    "    \"Gradient Boost\": (GradientBoostingClassifier(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }),\n",
    "    \"XGBoost\": (xgb.XGBClassifier(eval_metric='mlogloss'), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }),\n",
    "    \"ANN\": (MLPClassifier(max_iter=1000), {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "        'activation': ['relu', 'tanh', 'logistic'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    })\n",
    "}\n",
    "\n",
    "summary_results = []  # List to store the results of each model\n",
    "feature_files = [f\"Feature_W{w}_Olap{o}.csv\" for w in [100, 200, 300, 400, 500] for o in [0, 50]]  # List of feature files to process\n",
    "base_path = \"./data\"  # Path to feature files\n",
    "# ChatGPT came up with this idea to make it more guaranteed that each is used at least once\n",
    "used_kfold_once = False  # Flag to track if KFold cross-validation has been used once\n",
    "used_stratkfold_once = False  # Flag to track if StratifiedKFold cross-validation has been used once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0521b9a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each feature file\n",
    "print(2 + 2)\n",
    "for file in feature_files:\n",
    "    path = os.path.join(base_path, file)  # Make path\n",
    "    if not os.path.exists(path):  # Check if file exists\n",
    "        print(f\"File not found: {file}\")\n",
    "        continue  # Skip to next file if this one is not found\n",
    "\n",
    "    df = pd.read_csv(path)  # Read the CSV file into a Pandas DataFrame\n",
    "    X = df.drop(columns=[\"activity_type\", \"expID\", \"window_size\", \"overlap\", \"start_index\"])  # Drop non-feature columns\n",
    "    y = df[\"activity_type\"]  # Target variable (activity type)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    label_names = label_encoder.classes_\n",
    "    X_scaled = StandardScaler().fit_transform(X)  # Scale the features\n",
    "\n",
    "    # Loop through different test set sizes\n",
    "    for test_size in [0.2, 0.3]:\n",
    "        split_label = \"80/20\" if test_size == 0.2 else \"70/30\"  # Label if I want 80/20 or 70/30 split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y_encoded, test_size=test_size, stratify=y_encoded, random_state=42  # Split data into training and testing sets\n",
    "        )\n",
    "\n",
    "        # Loop through each model\n",
    "        for name, (model, params) in models.items():\n",
    "            # Determine cross-validation method\n",
    "            if not used_stratkfold_once:  # Use StratifiedKFold for the first time\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified K-Fold cross-validator\n",
    "                used_stratkfold_once = True\n",
    "                cv_type = \"StratifiedKFold\"\n",
    "            elif not used_kfold_once:  # Use KFold for the second time\n",
    "                cv = KFold(n_splits=5, shuffle=True, random_state=42)  # K-Fold cross-validator\n",
    "                used_kfold_once = True\n",
    "                cv_type = \"KFold\"\n",
    "            else:  # Use StratifiedKFold for the rest\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                cv_type = \"StratifiedKFold\"\n",
    "\n",
    "            # Perform Randomized Search Cross-Validation for hyperparameter tuning\n",
    "            grid = RandomizedSearchCV(\n",
    "                estimator=model,  \n",
    "                param_distributions=params,  \n",
    "                n_iter=5,  \n",
    "                cv=cv,  \n",
    "                scoring='accuracy',  \n",
    "                n_jobs=-1,  \n",
    "                random_state=42  \n",
    "            )\n",
    "            grid.fit(X_train, y_train)  # Fit RandomizedSearchCV\n",
    "            best_model = grid.best_estimator_  # Get the best model from search\n",
    "            y_pred = best_model.predict(X_test)  # Make predictions on test set\n",
    "            y_proba = best_model.predict_proba(X_test) if hasattr(best_model, \"predict_proba\") else None  # Get predicted probabilities\n",
    "\n",
    "            # Calculate relevant values\n",
    "            acc = accuracy_score(y_test, y_pred)  \n",
    "            prec = precision_score(y_test, y_pred, average='weighted')  \n",
    "            rec = recall_score(y_test, y_pred, average='weighted')  \n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')  \n",
    "            roc = roc_auc_score(y_test, y_proba, multi_class='ovr') if y_proba is not None else None  \n",
    "\n",
    "            # Do cross-validation to get average performance values\n",
    "            cv_scores = cross_validate(best_model, X_scaled, y_encoded, cv=cv, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "            acc_cv = cv_scores['test_accuracy'].mean()  # Mean CV accuracy\n",
    "            prec_cv = cv_scores['test_precision_weighted'].mean()  # Mean CV precision\n",
    "            rec_cv = cv_scores['test_recall_weighted'].mean()  # Mean CV recall\n",
    "            f1_cv = cv_scores['test_f1_weighted'].mean()  # Mean CV F1-score\n",
    "\n",
    "            class_report = classification_report(y_test, y_pred, output_dict=True)  # Classification report\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)  # Confusion matrix\n",
    "\n",
    "            # Store results in summary list\n",
    "            summary_results.append({\n",
    "                'Feature File': file,\n",
    "                'Model': name,\n",
    "                'Split': split_label,\n",
    "                'CV Type': cv_type,\n",
    "                'Best Params': grid.best_params_,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1 Score': f1,\n",
    "                'ROC AUC': roc,\n",
    "                'CV Accuracy': acc_cv,\n",
    "                'CV Precision': prec_cv,\n",
    "                'CV Recall': rec_cv,\n",
    "                'CV F1 Score': f1_cv,\n",
    "                'Classification Report': class_report,\n",
    "                'Confusion Matrix': conf_matrix\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19384a",
   "metadata": {},
   "source": [
    "## Summary / Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed391529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      "Final Summary (Top 20 Rows):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Full results saved to: ./final_model_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Display and save final summary\n",
    "summary_df = pd.DataFrame(summary_results)  # Convert summary list to DataFrame\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', 100)  \n",
    "print(\"\\nFinal Summary (Top 20 Rows):\")\n",
    "print(summary_df.head(20))  # Print first 20 rows of summary\n",
    "csv_path = \"./final_model_summary.csv\"  # Path to save summary CSV\n",
    "summary_df.to_csv(csv_path, index=False)  # Save summary to a CSV file\n",
    "print(f\"Full results saved to: {csv_path}\")  # Print save path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ca286",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e03502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average CV scores heatmap\n",
    "summary_avg = summary_df.groupby(\"Model\")[[\"CV Accuracy\", \"CV Precision\", \"CV Recall\", \"CV F1 Score\"]].mean().sort_values(\"CV Accuracy\", ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(summary_avg, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "plt.title(\"Average Cross-Validation Metrics per Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for best model overall\n",
    "best_result = summary_df.loc[summary_df[\"CV Accuracy\"].idxmax()]\n",
    "best_matrix = best_result[\"Confusion Matrix\"]\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(best_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title(f\"Best Model Confusion Matrix:\\n{best_result['Model']} ({best_result['Feature File']})\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
